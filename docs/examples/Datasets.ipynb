{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with MillionTrees datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TreePoints', 'TreeBoxes', 'TreePolygons']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'examples':\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "import milliontrees\n",
    "from torchvision import transforms\n",
    "\n",
    "# List available datasets\n",
    "print(milliontrees.benchmark_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general workflow is to \n",
    "1. Select and optionally download a dataset\n",
    "2. Load the train and test splits from the dataset\n",
    "3. Create the dataloader, with optional additional transforms, for how to preprocess images, and optionally augment, input images and metadata\n",
    "4. Use these dataloaders to train models in native pytorch or pytorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and optionally download a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the box dataset\n",
    "from milliontrees import get_dataset\n",
    "\n",
    "dataset = get_dataset(\"TreeBoxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test splits from the dataset\n",
    "\n",
    "Datasets are split into groups of images based on task. For example, 'train' versus 'test' or 'zero_shot_train' and 'zero_shot_test'. The get_subset function has a 'frac' argument to subsample the data during rapid testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length: 2\n",
      "Image shape: torch.Size([3, 448, 448]), Image type: <class 'torch.Tensor'>\n",
      "Targets keys: dict_keys(['y', 'labels']), Label type: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benweinstein/Documents/MillionTrees/src/milliontrees/datasets/milliontrees_dataset.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  metadata = torch.tensor(self.metadata_array[idx])\n"
     ]
    }
   ],
   "source": [
    "from milliontrees.common.data_loaders import get_train_loader\n",
    "\n",
    "train_dataset = dataset.get_subset(\"train\")\n",
    "\n",
    "# View the first image in the dataset\n",
    "metadata, image, targets = train_dataset[0]\n",
    "print(f\"Metadata length: {len(metadata)}\")\n",
    "print(f\"Image shape: {image.shape}, Image type: {type(image)}\")\n",
    "print(f\"Targets keys: {targets.keys()}, Label type: {type(targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You are loading the entire dataset. Consider using dataset.get_subset('train') for a portion of the dataset if intended.\n",
      "Targets is a list of dictionaries with the following keys:  dict_keys(['y', 'labels'])\n",
      "Image shape: torch.Size([2, 3, 448, 448]), Image type: <class 'torch.Tensor'>\n",
      "Annotation shape of the first image: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_train_loader(\"standard\", train_dataset, batch_size=2)\n",
    "\n",
    "# Show one batch of the loader\n",
    "for metadata, image, targets in train_loader:\n",
    "    print(\"Targets is a list of dictionaries with the following keys: \",\n",
    "          targets[0].keys())\n",
    "    print(f\"Image shape: {image.shape}, Image type: {type(image)}\")\n",
    "    print(f\"Annotation shape of the first image: {targets[0]['y'].shape}\")\n",
    "    break  # Just show the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m: torch.tensor([[\u001b[32m30\u001b[39m, \u001b[32m70\u001b[39m, \u001b[32m35\u001b[39m, \u001b[32m75\u001b[39m]]),\n\u001b[32m     14\u001b[39m              \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: torch.tensor([\u001b[32m0\u001b[39m]),\n\u001b[32m     15\u001b[39m              \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m: torch.tensor([\u001b[32m0.54\u001b[39m])} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(images.shape[\u001b[32m0\u001b[39m])]\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# For the sake of this example, we will make up some predictions to show format\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_metadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_y_true\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MillionTrees/src/milliontrees/datasets/milliontrees_dataset.py:481\u001b[39m, in \u001b[36mMillionTreesSubset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     metadata, x, targets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_name == \u001b[33m'\u001b[39m\u001b[33mTreeBoxes\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    484\u001b[39m         augmented = \u001b[38;5;28mself\u001b[39m.transform(image=x,\n\u001b[32m    485\u001b[39m                                    bboxes=targets[\u001b[38;5;28mself\u001b[39m.geometry_name],\n\u001b[32m    486\u001b[39m                                    labels=targets[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MillionTrees/src/milliontrees/datasets/milliontrees_dataset.py:38\u001b[39m, in \u001b[36mMillionTreesDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Any transformations are handled by the WILDSSubset\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# since different subsets (e.g., train vs test) might have different transforms\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     y_indices = \u001b[38;5;28mself\u001b[39m._input_lookup[\u001b[38;5;28mself\u001b[39m._input_array[idx]]\n\u001b[32m     40\u001b[39m     y = torch.tensor(\u001b[38;5;28mself\u001b[39m.y_array[y_indices])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MillionTrees/src/milliontrees/datasets/TreeBoxes.py:212\u001b[39m, in \u001b[36mTreeBoxesDataset.get_input\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    209\u001b[39m img_path = os.path.join(\u001b[38;5;28mself\u001b[39m._data_dir / \u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m /\n\u001b[32m    210\u001b[39m                         \u001b[38;5;28mself\u001b[39m._input_array[idx])\n\u001b[32m    211\u001b[39m img = Image.open(img_path)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m img = np.array(\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m) / \u001b[32m255\u001b[39m\n\u001b[32m    213\u001b[39m img = np.array(img, dtype=np.float32)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/PIL/Image.py:982\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    980\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MillionTrees/lib/python3.11/site-packages/PIL/ImageFile.py:385\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    382\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    384\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from milliontrees.common.data_loaders import get_eval_loader\n",
    "import torch\n",
    "\n",
    "test_dataset = dataset.get_subset(\"train\")\n",
    "test_loader = get_eval_loader(\"standard\", test_dataset, batch_size=16)\n",
    "\n",
    "# Show one batch of the loader\n",
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "all_metadata = []\n",
    "\n",
    "\n",
    "def predictor(images):\n",
    "    return [{\n",
    "        'y': torch.tensor([[30, 70, 35, 75]]),\n",
    "        'label': torch.tensor([0]),\n",
    "        'score': torch.tensor([0.54])\n",
    "    } for _ in range(images.shape[0])]\n",
    "\n",
    "\n",
    "# For the sake of this example, we will make up some predictions to show format\n",
    "for metadata, images, targets in test_loader:\n",
    "    all_metadata.append(metadata)\n",
    "    all_y_true.append(targets)\n",
    "    all_y_pred.append(predictor(images))\n",
    "\n",
    "# Evaluate\n",
    "dataset.eval(all_y_pred, all_y_true, all_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MillionTrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
